{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import konlpy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일단 개수를 줄여서 돌려보자.\n",
    "## train_x 는 series of sentences\n",
    "## train_y 는 serise of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size  = 15000  # size 상수\n",
    "const_tfidf = 1000   #구할 tfidf상위 단어의 개수\n",
    "const_kai   = 1000   #구할 카이제곱상위 단어의 개수\n",
    "# test_size   = int(train_size/3)\n",
    "# train_x, train_y = train_df[\"document\"][:train_size], train_df[\"label\"][:train_size]\n",
    "# test_x, test_y = test_df[\"document\"][:test_size], test_df[\"label\"][:test_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 긍/ 부정 문서 카이제곱 상위단어 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCount(data, w):\n",
    "    cnt = 0;\n",
    "    for s in data:\n",
    "        if w in s:\n",
    "            cnt += 1\n",
    "    return cnt;\n",
    "\n",
    "def getKai(data, w, c):\n",
    "    A = wordCount(data[c], w)\n",
    "    B = wordCount(data[(c+1)%2], w)\n",
    "    C = len(data[c]) - A\n",
    "    D = len(data[(c+1)%2]) - B\n",
    "    # 분모가 0이면 0을 리턴함.\n",
    "    if ((A+B)*(A+C)*(B+D)*(C+D)) == 0:\n",
    "        return 0\n",
    "    return ((A+B+C+D)*((A*D-B*C)*(A*D-B*C))) / ((A+B)*(A+C)*(B+D)*(C+D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('X_sample.pickle', 'rb') as f:\n",
    "    train_x = pickle.load(f) # 단 한줄씩 읽어옴\n",
    "with open('y_sample.pickle', 'rb') as f:\n",
    "    train_y  = pickle.load(f) # 단 한줄씩 읽어옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_dict = tfidf_dict[:const_tfidf]\n",
    "train_x = list(train_x)\n",
    "train_y = list(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for s in doc:\n",
    "    for w in s:\n",
    "        words.append(w)\n",
    "words = list(set(words))\n",
    "data = [list(train_x[train_y == 0]), list(train_x[train_y == 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11060/11060 [00:00<00:00, 777608.33it/s]\n"
     ]
    }
   ],
   "source": [
    "kai_dict = []\n",
    "for w in tqdm(words):\n",
    "    kai_dict.append(getKai(data, w, 0))\n",
    "kaiRate = pd.DataFrame()\n",
    "kaiRate['words'] = words\n",
    "kaiRate.index = words\n",
    "kaiRate['word_kai'] = kai_dict\n",
    "kaiRate = kaiRate.sort_values(by=['word_kai'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaiRate = kaiRate[:const_kai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaiRate = list(kaiRate[\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "def create_vec_count(x_data, voca):\n",
    "    vec = np.ndarray((len(x_data), len(voca)))\n",
    "    for i, s in enumerate(tqdm(x_data)): \n",
    "        for j, w in enumerate(voca):\n",
    "            vec[i][j] = s.count(w)\n",
    "    return sparse.csr_matrix(vec)\n",
    "#     return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원핫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vec_sparse(x_data, voca):\n",
    "    vec = np.ndarray((len(x_data), len(voca)))\n",
    "    for i, s in enumerate(tqdm(x_data)): \n",
    "        for j, w in enumerate(voca):\n",
    "            if str(w) in str(s):\n",
    "                vec[i][j] = 1\n",
    "            else:\n",
    "                vec[i][j] = 0\n",
    "    return sparse.csr_matrix(vec)\n",
    "#     return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:17<00:00, 72.98it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 835.35it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-cc50ba0a6adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_bone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vec_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_kai_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vec_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkaiRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_bone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vec_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_kai_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vec_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkaiRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_x' is not defined"
     ]
    }
   ],
   "source": [
    "train_bone = create_vec_sparse(train_x, words)\n",
    "train_kai_sparse = create_vec_sparse(train_x, kaiRate)\n",
    "# test_bone = create_vec_sparse(test_x, words)\n",
    "# test_kai_sparse = create_vec_sparse(test_x, kaiRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터를 합친다.\n",
    "def merge_sparse(a, b):\n",
    "    return sparse.csr_matrix(np.hstack((a.toarray(), b.toarray())))\n",
    "def merge_np(a, b):\n",
    "    return np.concatenate((a, b), axis = 1)\n",
    "\n",
    "\n",
    "train_with_kai_sprs = merge_sparse(train_bone, train_kai_sparse)\n",
    "test_with_kai_sprs = merge_sparse(test_bone, test_kai_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifier_tfidf_sprs_1 = MultinomialNB()\n",
    "\n",
    "classifier_tfidf_sprs_1.fit(train_with_kai_sprs, train_y)\n",
    "\n",
    "# pred_tfidf_sprs_1 = classifier_tfidf_sprs_1.predict(test_with_kai_sprs).tolist()\n",
    "\n",
    "print('Accuracy1: %.10f' % accuracy_score(test_y, pred_tfidf_sprs_1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
